{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnOOxiktCeT2",
        "outputId": "ba4e43b4-d127-42b4-d6d8-96ed5f9ac62f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['Topic sentences are similar to mini thesis statements.\\\n",
        "        Like a thesis statement, a topic sentence has a specific \\\n",
        "        main point. Whereas the thesis is the main point of the essay',\\\n",
        "        'the topic sentence is the main point of the paragraph.\\\n",
        "        Like the thesis statement, a topic sentence has a unifying function. \\\n",
        "        But a thesis statement or topic sentence alone doesnâ€™t guarantee unity.', \\\n",
        "        'An essay is unified if all the paragraphs relate to the thesis,\\\n",
        "        whereas a paragraph is unified if all the sentences relate to the topic sentence.']\n",
        "        "
      ],
      "metadata": {
        "id": "TRZRAFqICufP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "words = []\n",
        "\n",
        "for sentence in text:\n",
        "  x = [i.lower() for i in word_tokenize(sentence) if i.isalpha()]\n",
        "  sentences.append(x)\n",
        "  for word in x:\n",
        "    if word not in words:\n",
        "      words.append(word)\n",
        "\n",
        "words = set(words)\n",
        "\n",
        "total_documents = len(sentences) # for idf"
      ],
      "metadata": {
        "id": "JxrsKcffDIA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index each word of words for creating the final tfidf numpy vector (array)\n",
        "word_index = {}\n",
        "i = 0\n",
        "for word in words:\n",
        "  word_index[word] = i\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "scbf63wMKHk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of documents containing the given word\n",
        "def docs_word_count(sentences):\n",
        "  word_count = {}\n",
        "  for word in words:\n",
        "    word_count[word] = 0\n",
        "    for sentence in sentences:\n",
        "      if word in sentence:\n",
        "        word_count[word] += 1\n",
        "\n",
        "  return word_count\n",
        "\n",
        "word_count = docs_word_count(sentences) # for idf"
      ],
      "metadata": {
        "id": "PeNAcJK_K4OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate term frequency\n",
        "\n",
        "def term_frequency(sentence, word):\n",
        "  n = len(sentence)\n",
        "  occurence = len([token for token in sentence if token == word])\n",
        "  return occurence/n"
      ],
      "metadata": {
        "id": "kldGFOmMNNLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate inverse document frequency\n",
        "def inverse_document_frequency(word):\n",
        "  try:\n",
        "    word_occurence = word_count[word] + 1\n",
        "  except:\n",
        "    word_occurence = 1\n",
        "  \n",
        "  return np.log(total_documents / word_occurence)"
      ],
      "metadata": {
        "id": "G60PZ7OJOoAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(sentence):\n",
        "  tfidf_vector = np.zeros(len(words),)\n",
        "  for word in sentence:\n",
        "    tf = term_frequency(sentence, word)\n",
        "    idf = inverse_document_frequency(word)\n",
        "    tfidf_value = tf * idf\n",
        "    tfidf_vector[word_index[word]] = tfidf_value\n",
        "\n",
        "  return tfidf_vector\n",
        "\n"
      ],
      "metadata": {
        "id": "hduZmX79PnCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply tfidf to text\n",
        "vectors = []\n",
        "for sentence in sentences:\n",
        "  vec = tfidf(sentence)\n",
        "  vectors.append(vec)\n",
        "\n",
        "print(vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp--O9M7STz5",
        "outputId": "c970b86b-d3ce-422d-a247-b7ab90a05ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.0135155   0.          0.          0.         -0.02876821  0.\n",
            "  0.         -0.02876821  0.          0.          0.          0.\n",
            "  0.          0.          0.0135155   0.          0.0135155   0.\n",
            " -0.02876821 -0.0095894   0.          0.         -0.0095894   0.\n",
            "  0.          0.          0.         -0.0191788   0.          0.\n",
            "  0.          0.          0.0135155   0.0135155   0.          0.\n",
            "  0.        ]\n"
          ]
        }
      ]
    }
  ]
}