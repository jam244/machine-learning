{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_features = list(dataset.select_dtypes(include=numerics).columns) \n",
    "categorical_features = list(dataset.select_dtypes(include=['object']).columns) \n",
    "#numerical_features=numerical_features[1:-1]\n",
    "print(numerical_features)\n",
    "print(categorical_features)\n",
    "dataset=dataset[numerical_features]\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, -1].values\n",
    "data= dataset.drop(['Id', 'SalePrice'], axis=1)\n",
    "display(data)\n",
    "X = data.iloc[:, :].values\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(X.shape[0] * 0.75)\n",
    "\n",
    "x_train = X[:split,:]\n",
    "y_train = y[:split]\n",
    "\n",
    "x_test = X[split:,:]\n",
    "y_test = y[split:]\n",
    "\n",
    "val_split = int(x_test.shape[0] * 0.50)\n",
    "x_val = x_test[:val_split,:]\n",
    "y_val = y_test[:val_split]\n",
    "\n",
    "x_test = x_test[val_split:,:]\n",
    "y_test = y_test[val_split:]\n",
    "\n",
    "print('Training set size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])\n",
    "print('Validation set size:', x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, y):\n",
    "    return sqrt(mean_squared_error(pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(x, y, n_features):\n",
    "    \"\"\"Given a dataset and its target values, this finds the optimal combination\n",
    "    of feature and split point that gives the maximum information gain.\"\"\"\n",
    "    \n",
    "   \n",
    "    # Best thus far, initialised to a dud that will be replaced immediately...\n",
    "    best = {'minimize_variance' : np.inf}\n",
    "    \n",
    "    # Random subspace\n",
    "    features = np.random.randint(x.shape[1], size =  n_features)\n",
    " \n",
    "    # Loop every possible split of every dimension...\n",
    "    for feature in features:\n",
    "        for split in np.unique(x[:,feature]):\n",
    "            #print(split)\n",
    "            left_indices = []\n",
    "            right_indices = []\n",
    "            \n",
    "            for index, row in enumerate(x):\n",
    "                if row[feature] <= split:\n",
    "                    left_indices.append(index)\n",
    "                else:\n",
    "                    right_indices.append(index)\n",
    "                    \n",
    "            n = float(len(left_indices) + len(right_indices))\n",
    "            nl = float(len(left_indices))\n",
    "            nr = float(len(right_indices))\n",
    "            var_r = 0.0\n",
    "            var_l = 0.0\n",
    "            if len(y[left_indices]) > 0:\n",
    "                var_l = np.var(y[left_indices])\n",
    "            \n",
    "            if len(y[right_indices]) > 0:\n",
    "                var_r = np.var(y[right_indices])\n",
    "            \n",
    "            minimize_variance = float(nl/n) * var_l + float(nr/n) * var_r\n",
    "            \n",
    "            if minimize_variance < best['minimize_variance']:\n",
    "                #print(minimize_variance)\n",
    "                best = {'feature' : feature,\n",
    "                        'split' : split,\n",
    "                        'minimize_variance' : minimize_variance, \n",
    "                        'left_indices' : left_indices,\n",
    "                        'right_indices' : right_indices}\n",
    "    return best\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function find_split() allows us to find the optimal feature and the best value to split the data into two chunks (on its own it is the decision stump algorithm). Applying this to the original data set splits it into two new data sets. We can then repeat this on both of the new data sets to get four data sets, and so on. This recursion builds a decision tree. It needs a stopping condition, to prevent it dividing the data forever, here we will use two:\n",
    "\n",
    "Maximum depth: The tree is limited to be no deeper than a provided limit.\n",
    "Perfection: If a node contains only one class then it does not make sense to split it further.\n",
    "We provide the function build_tree(x, y, max_depth) below to construct a tree. The inputs are:\n",
    "\n",
    "The data matrix of features, x in R^None. n is the number of data points and d is the feature dimensionality.\n",
    "y, a column vector of size n containing the target value for each data point in x.\n",
    "The maximum depth of the tree, max_depth.\n",
    "The output of this function is a dictionary. If it has generated a leaf node then the keys are:\n",
    "\n",
    "'leaf' : True\n",
    "'mean' : Mean of the values of y to assign to exemplars that land here.\n",
    "If it has generated a split node then the keys are:\n",
    "\n",
    "'leaf' : False\n",
    "'feature': The feature to apply the split to.\n",
    "'split': The split to test the exemplars feature with.\n",
    "'minimize_variance': The variance minimized of this split.\n",
    "'left' : The left subtree, for exemplars where x[feature_index]<=split\n",
    "'right' : The right subtree, for exemplars where x[feature_index]>split\n",
    "Note how this structure is compatable with the one returned by find_split() above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(x, y, max_depth = np.inf, n_features=6):\n",
    "    # Check if either of the stopping conditions have been reached. If so generate a leaf node...\n",
    "    move = find_split(x, y, n_features)\n",
    "    if max_depth==1 or len(move['left_indices']) == 0  or len(move['right_indices']) == 0:\n",
    "        # Generate a leaf node...\n",
    "        return {'leaf' : True, 'mean' : np.mean(y)}\n",
    "    \n",
    "    else:\n",
    "        left = build_tree(x[move['left_indices'],:], y[move['left_indices']], max_depth - 1, n_features)\n",
    "        right = build_tree(x[move['right_indices'],:], y[move['right_indices']], max_depth - 1, n_features)\n",
    "        \n",
    "        return {'leaf' : False,\n",
    "                'feature' : move['feature'],\n",
    "                'split' : move['split'],\n",
    "                'minimize_variance' : move['minimize_variance'],\n",
    "                'left' : left,\n",
    "                'right' : right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the tree we should be able to predict the mean of a sample. We do that by propagating the sample through the tree, i.e. we check all the splitting conditions until the sample falls in a leaf node, in which case the mean of the leaf node is attributed to the sample.\n",
    "\n",
    "We further generalize the prediction function above to the case where we have a data matrix R^None representing many data points. the function predict(tree, samples) bellow takes as input the constructed tree and a data array then returns an array containing the predictions for all the samples in our input data array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, samples):\n",
    "    \"\"\"Predicts mean for every entry of a data matrix.\"\"\"\n",
    "    ret = np.empty(samples.shape[0], dtype=float)\n",
    "    ret.fill(0.0)\n",
    "    indices = np.arange(samples.shape[0])\n",
    "    \n",
    "    def tranverse(node, indices):\n",
    "        nonlocal samples\n",
    "        nonlocal ret\n",
    "        \n",
    "        if node['leaf']:\n",
    "            ret[indices] = node['mean']\n",
    "        \n",
    "        else:\n",
    "            going_left = samples[indices, node['feature']] <= node['split']\n",
    "            left_indices = indices[going_left]\n",
    "            right_indices = indices[np.logical_not(going_left)]\n",
    "            \n",
    "            if left_indices.shape[0] > 0:\n",
    "                tranverse(node['left'], left_indices)\n",
    "                \n",
    "            if right_indices.shape[0] > 0:\n",
    "                tranverse(node['right'], right_indices)\n",
    "    \n",
    "    tranverse(tree, indices)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_predict(trees, dataset):\n",
    "    predictions = np.zeros((dataset.shape[0], len(trees)), dtype=float)\n",
    "\n",
    "    for i in range (len(trees)):\n",
    "        #print('------------- Tree -----------------------')\n",
    "        prediction = predict(trees[i], dataset)\n",
    "        predictions[:, i] = prediction\n",
    "    \n",
    "    return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(n):\n",
    "    draws = np.random.choice(len(n), size =  len(n))\n",
    "    return draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_num_trees = 5 # Hard coded to due to fixed computational budget\n",
    "#best_feature_number = 6 # Hardcoded to sqrt of number of feautures due to fixed computational budget\n",
    "\n",
    "def train(x, y, max_depth, n_trees, n_features):\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'pickles')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory)\n",
    "    trees = []\n",
    "    for i in range(n_trees):\n",
    "        filename = 'tree_' + str(len(x)) + '_' + str(max_depth) + '_' + str(n_trees) + '_' + str(n_features) + '_' + str(i)\n",
    "        for root, dirs, files in os.walk(final_directory):\n",
    "            if filename in files:\n",
    "                filename = os.path.join(root, filename)\n",
    "                pf = open(filename,\"rb\")\n",
    "                tree = pickle.load(pf)\n",
    "                print(\"Found tree in: \" + filename)\n",
    "            else:\n",
    "                bootstrap_draws = bootstrap(x)\n",
    "                #bootstrap_draws = np.arange(len(x))\n",
    "                #print(bootstrap_draws)\n",
    "                tree = build_tree(x[bootstrap_draws], y[bootstrap_draws], max_depth, n_features)\n",
    "                filehandler = open(os.path.join(final_directory, filename), 'wb')\n",
    "                pickle.dump(tree, filehandler)\n",
    "                print(\"storing: \" + filename)\n",
    "        trees.append(tree)\n",
    "    print(f'----- trees:{len(trees)} Depth:{max_depth} Features:{n_features} ------')\n",
    "    return trees\n",
    "\n",
    "# Train\n",
    "print(datetime.now())\n",
    "temp_forest = train(x_train, y_train, np.inf, 10, 6)\n",
    "print(datetime.now())\n",
    "y_train_pred = forest_predict(temp_forest, x_train)\n",
    "train_mse = rmse(y_train,y_train_pred)\n",
    "print(\"train_mse: {:.2f}\".format(train_mse))\n",
    "\n",
    "r2_score(y_train, y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(x_t, y_t, x_val, y_val):\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_max_depth = 0\n",
    "    best_num_trees = 0\n",
    "    best_num_features = 0\n",
    "    depths = [5, 10, 20, 30, 40, 60,100]\n",
    "    num_trees = [5, 10, 20, 30, 40, 50]\n",
    "    num_features = [15, 30, 60, 120, 240]\n",
    "    rmse = []\n",
    "    for num_tree in num_trees:\n",
    "        for num_feature in num_features:\n",
    "            for depth in depths:\n",
    "                temp_forest = train(x_t, y_t, depth, num_tree, num_feature)\n",
    "                y_val_pred = forest_predict(temp_forest, x_val)\n",
    "                d_val = np.subtract(y_val, y_val_pred)\n",
    "                temp_val_rmse = np.sqrt(np.mean(d_val**2))\n",
    "                rmse.append(temp_val_rmse)\n",
    "                print(\"val_rmse: {:.2f}\".format(temp_val_rmse))\n",
    "                if temp_val_rmse < best_val_rmse:\n",
    "                    best_max_depth = depth\n",
    "                    best_num_trees = num_tree\n",
    "                    best_num_features = num_feature\n",
    "                    best_val_rmse = temp_val_rmse\n",
    "    print(\"best_max_depth\", best_max_depth)\n",
    "    print(\"best_num_trees\", best_num_trees)\n",
    "    print(\"best_num_features\", best_num_features)\n",
    "    return best_max_depth, best_num_trees, best_num_features, best_val_rmse\n",
    "\n",
    "#print(datetime.now())\n",
    "#best_max_depth, best_num_trees, best_num_features, best_val_rmse = find_best_hyperparameters(x_train, y_train, x_val, y_val)\n",
    "#print('The best max_depth is {} and the corresponding val RMSE is {:.2f} .'.format(best_max_depth, best_val_rmse))\n",
    "#print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_depth = 0\n",
    "best_num_trees = 0\n",
    "best_num_features = 0\n",
    "#Test\n",
    "print(datetime.now())\n",
    "best_forest = train(x_train, y_train, np.inf, 10, 6)\n",
    "y_train_pred = forest_predict(best_forest, x_train)\n",
    "train_mse = np.square(np.subtract(y_train,y_train_pred)).mean()\n",
    "print('Train MSE score : {:.2f}'.format(train_mse))\n",
    "y_test_pred = forest_predict(best_forest, x_test)\n",
    "test_mse = np.square(np.subtract(y_test,y_test_pred)).mean()\n",
    "print('Test MSE score : {:.2f}'.format(test_mse))\n",
    "print(datetime.now())\n",
    "\n",
    "r2_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit to kaggle\n",
    "\n",
    "test_data=pd.read_csv('test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=test_data[numerical_features[:-1]]\n",
    "\n",
    "data= dataset.drop(['Id'], axis=1)\n",
    "display(data)\n",
    "x_sub = data.iloc[:, :].values\n",
    "print(dataset.head())\n",
    "y_sub_pred = forest_predict(best_forest, x_sub)\n",
    "\n",
    "print(y_sub_pred)\n",
    "\n",
    "submission = pd.DataFrame({'Id': dataset.Id, 'SalePrice': y_sub_pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
